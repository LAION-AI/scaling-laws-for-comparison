job_id_regexp: "Job Id:(\\d+)"
cmd: "sbatch {sbatch_script}"
check_interval_secs: 600
partition: booster
account: transfernetx
beta1: 0.9
beta2: 0.95
wd: 0.2
time_minutes: 360
grad_clip_norm: 1
lr-scheduler: const
logs: "/p/data1/mmlaion/cherti1/open_clip_scaling/logs/const_lr"
pretrained: ""
experiments:
    model: [ViT-S-32-alt, ViT-S-32, ViT-M-32, ViT-M-32-alt, ViT-S-16, ViT-S-16-alt, ViT-B-32, ViT-M-16, ViT-M-16-alt, ViT-B-16, ViT-L-14, ViT-H-14]
    nodes: 64
    train_num_samples: 12_800_000
    lr: [2e-3, 1e-3]
    batch_size: 354
    siglip: false
    max_epochs: 1000
    warmup: 4000
phase:
    - pretrain:
        lr_scheduler: const
        lr_cooldown_end: 0.0
        lr_cooldown_power: 1.0
        epochs_cooldown: 0
        folder_name: "{exp_name}"
        epochs: "{max_epochs}"
        phase_params: ""
        resume: latest
        mode:
            - train:
                template: /p/project/laionize/cherti1/open_clip_scaling/configs/const_lr/train.sbatch
                sbatch_script: "/p/project/laionize/cherti1/open_clip_scaling/sbatch_scripts/const_lr/{exp_name}_train.sbatch"
                output_file: "{logs}/{exp_name}/slurm_train.out"
                termination_cmd: 'let last={epochs}-1;grep "Train Epoch: $last.*100%" {logs}/{folder_name}/out.log|wc -l'
            - eval:
                template: /p/project/laionize/cherti1/open_clip_scaling/configs/const_lr/eval.sbatch
                sbatch_script: "/p/project/laionize/cherti1/open_clip_scaling/sbatch_scripts/const_lr/{exp_name}_eval.sbatch"
                output_file: "{logs}/{exp_name}/slurm_eval.out"
                eval_nodes: 1
                start_condition_cmd: "nc=`ls {logs}/{exp_name}/checkpoints/epoch_[0-9]*.pt|wc -l`;ne=`ls {logs}/{exp_name}/checkpoints/*.json|wc -l`;echo $(( (nc*2-ne) > 0 ))" # imagenet + coco = 2
                termination_cmd: "ne=`ls {logs}/{exp_name}/checkpoints/*.json|wc -l`;echo $(( (ne) == {epochs}*2 ))" # num of jsons = (num epochs) * (imagenet + coco = 2)
    - cooldown:
        lr_scheduler: const-cooldown
        lr_cooldown_end: 0.0
        lr_cooldown_power: 1.0
        samples_seen_scale:
            - 128M:
                epochs_cooldown: 2
                cooldown_checkpoint: 8
            - 1.28B:
                epochs_cooldown: 25
                cooldown_checkpoint: 75
            - 3B:
                epochs_cooldown: 58
                cooldown_checkpoint: 176
            - 12.8B:
                epochs_cooldown: 250
                cooldown_checkpoint: 750
        epochs: "$(({cooldown_checkpoint}+{epochs_cooldown}))"
        checkpoint: "{logs}/{exp_name}/checkpoints/epoch_{cooldown_checkpoint}.pt"
        folder_name: "{exp_name}/cooldown_s{samples_seen_scale}"
        phase_params: "{samples_seen_scale}"
        mode:
            - train:
                template: /p/project/laionize/cherti1/open_clip_scaling/configs/const_lr/train.sbatch     
                sbatch_script: "/p/project/laionize/cherti1/open_clip_scaling/sbatch_scripts/const_lr/{exp_name}_cooldown_s{samples_seen_scale}_train.sbatch"
                output_file: "{logs}/{folder_name}/slurm_train.out"
                start_condition_cmd: "nc=`ls {checkpoint}|wc -l`;echo $(( (nc) > 0 ))"
                termination_cmd: 'let last=$(( {cooldown_checkpoint} + {epochs_cooldown} - 1 ));grep "Train Epoch: $last.*100%" {logs}/{folder_name}/out.log|wc -l'
                resume: "$(if [ -f '{logs}/{folder_name}/checkpoints/epoch_latest.pt' ]; then echo 'latest'; else echo '{checkpoint}'; fi)"
            - eval:
                template: /p/project/laionize/cherti1/open_clip_scaling/configs/const_lr/eval.sbatch
                sbatch_script: "/p/project/laionize/cherti1/open_clip_scaling/sbatch_scripts/const_lr/{exp_name}_cooldown_s{samples_seen_scale}_eval.sbatch"
                output_file: "{logs}/{folder_name}/slurm_eval.out"
                eval_nodes: 1
                start_condition_cmd: "nc=`ls {logs}/{folder_name}/checkpoints/epoch_[0-9]*.pt|wc -l`;ne=`ls {logs}/{folder_name}/checkpoints/*.json|wc -l`;echo $(( (nc*2-ne) > 0 ))" # imagenet + coco = 2
                termination_cmd: "ne=`ls {logs}/{folder_name}/checkpoints/*.json|wc -l`;echo $(( (ne) == {epochs_cooldown}*2 ))" # num of jsons = (num epochs) * (imagenet + coco = 2)
dataset: 
    - datacomp:
        train_data: "/p/data1/mmlaion/datacomp/datacomp_1B/flat/{0000000..0139827}.tar"
exp_name: "{dataset}_{model}_ep{max_epochs}_lr{lr}_b1_{beta1}_b2_{beta2}_wd{wd}_w{warmup}_gc{grad_clip_norm}_n{nodes}_bs{batch_size}_sig{siglip}"
name: "{exp_name}_{phase}{phase_params}_{mode}"
